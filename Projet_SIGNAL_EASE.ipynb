{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Projet : Système de recommandation de musiques"
      ],
      "metadata": {
        "id": "VX2TXx6Qt4De"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "import tarfile\n",
        "import h5py\n",
        "import pandas as pd\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Change the current working directory to the path of Google Cloud Drive\n",
        "path=\"/content/drive/MyDrive/Projet_SIGNAL/MillionSongSubset\"\n",
        "os.listdir(path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_lrSU6ewdPd",
        "outputId": "dd77205c-9248-4c89-d508-641e7411a360"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A',\n",
              " 'B',\n",
              " 'user_interactions.gsheet',\n",
              " 'recommendations.csv',\n",
              " 'user_interactions.csv',\n",
              " 'user_item_matrix.csv',\n",
              " 'songs_data.csv',\n",
              " 'songs_data_cleaned.csv',\n",
              " 'user_item_weighted_matrix.csv',\n",
              " 'filtered_recommendations.csv',\n",
              " 'train_triplets.txt',\n",
              " 'all_users_filtered.gsheet',\n",
              " 'all_users.csv',\n",
              " 'all_users_filtered.csv',\n",
              " 'user_interactions_with_duration_hotness.csv',\n",
              " 'user_item_matrix.npy',\n",
              " 'item_cooccurrence_matrix.npy',\n",
              " 'ease_weights.npy',\n",
              " 'filtered_recommendations_real_users.csv',\n",
              " 'user_item_interactions.csv']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chargement des fichiers dataset utilisateurs"
      ],
      "metadata": {
        "id": "Eu2kO9M2cjyK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Spécifiez le chemin de votre fichier\n",
        "file_path = '/content/drive/MyDrive/Projet_SIGNAL/MillionSongSubset/train_triplets.txt'\n",
        "\n",
        "# Lire le fichier dans un DataFrame\n",
        "df = pd.read_csv(file_path, sep='\\t', header=None, names=['user_id', 'song_id', 'play_count'])\n",
        "\n",
        "# Spécifiez le chemin de sortie pour le fichier CSV\n",
        "csv_output_path = '/content/drive/MyDrive/Projet_SIGNAL/MillionSongSubset/all_users.csv'\n",
        "\n",
        "# Sauvegarder le DataFrame dans un fichier CSV\n",
        "df.to_csv(csv_output_path, index=False)\n",
        "print('Finish')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQHhCztJNsVC",
        "outputId": "f7e15680-c870-4d37-da14-97ed20a8510b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finish\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Filtrage des utilisateurs"
      ],
      "metadata": {
        "id": "2dXL64JVcsLR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Charger les données des chansons\n",
        "songs_data_path = '/content/drive/MyDrive/Projet_SIGNAL/MillionSongSubset/songs_data_cleaned.csv'\n",
        "songs_df = pd.read_csv(songs_data_path)\n",
        "\n",
        "# Créer un ensemble avec les identifiants des chansons de votre sous-ensemble\n",
        "song_subset = set(songs_df['song_id'])\n",
        "\n",
        "# Charger les données des utilisateurs\n",
        "user_data_path = '/content/drive/MyDrive/Projet_SIGNAL/MillionSongSubset/all_users.csv'\n",
        "user_df = pd.read_csv(user_data_path)\n",
        "\n",
        "# Filtrer les lignes où song_id est dans song_subset\n",
        "filtered_df = user_df[user_df['song_id'].isin(song_subset)]\n",
        "\n",
        "# Compter le nombre de chansons uniques écoutées par chaque utilisateur\n",
        "user_song_counts = filtered_df.groupby('user_id')['song_id'].nunique()\n",
        "\n",
        "# Filtrer pour ne garder que les utilisateurs qui ont écouté plus de 5 chansons différentes\n",
        "filtered_users = user_song_counts[user_song_counts > 5].index\n",
        "\n",
        "# Créer un nouveau DataFrame avec seulement ces utilisateurs\n",
        "final_filtered_df = filtered_df[filtered_df['user_id'].isin(filtered_users)]\n",
        "\n",
        "# Sauvegarder le DataFrame final\n",
        "final_filtered_df.to_csv('/content/drive/MyDrive/Projet_SIGNAL/MillionSongSubset/all_users_filtered.csv', index=False)\n"
      ],
      "metadata": {
        "id": "5lqOQZkrNtZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Affichage nombre d'utilisateurs"
      ],
      "metadata": {
        "id": "O70-1OXQcz_s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Charger le fichier all_users_filtered.csv\n",
        "filtered_data_path = '/content/drive/MyDrive/Projet_SIGNAL/MillionSongSubset/all_users_filtered.csv'\n",
        "filtered_df = pd.read_csv(filtered_data_path)\n",
        "\n",
        "# Compter le nombre d'utilisateurs uniques\n",
        "unique_user_count = filtered_df['user_id'].nunique()\n",
        "\n",
        "print(f\"Nombre d'utilisateurs uniques : {unique_user_count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsU0I37INuct",
        "outputId": "5780411f-74a3-4df6-db88-6a3ecbead396"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nombre d'utilisateurs uniques : 9897\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prétraitement"
      ],
      "metadata": {
        "id": "1zpJ4AWyG4lr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import h5py\n",
        "import pandas as pd\n",
        "from multiprocessing import Pool, current_process\n",
        "\n",
        "root_path = '/content/drive/MyDrive/Projet_SIGNAL/MillionSongSubset'\n",
        "aggregated_data = []\n",
        "\n",
        "def read_h5_file(file_path):\n",
        "    with h5py.File(file_path, 'r') as file:\n",
        "        # Exemple d'extraction des données\n",
        "        song_id = file['metadata/songs']['song_id'][0].decode('UTF-8')\n",
        "        artist_name = file['metadata/songs']['artist_name'][0].decode('UTF-8')\n",
        "        song_hotttnesss = file['metadata/songs']['song_hotttnesss'][0]\n",
        "        duration = file['analysis/songs']['duration'][0]\n",
        "\n",
        "        # Ajouter les données extraites à la liste aggregated_data\n",
        "        aggregated_data.append({\n",
        "            'song_id': song_id,\n",
        "            'artist_name': artist_name,\n",
        "            'song_hotttnesss': song_hotttnesss,\n",
        "            'duration': duration\n",
        "        })\n",
        "        return {\n",
        "            'song_id': song_id,\n",
        "            'artist_name': artist_name,\n",
        "            'song_hotttnesss': song_hotttnesss,\n",
        "            'duration': duration\n",
        "        }\n",
        "\n",
        "def process_files(file_paths):\n",
        "    data = []\n",
        "    process_id = current_process().name  # Identifiant du processus\n",
        "    for index, file_path in enumerate(file_paths):\n",
        "        data.append(read_h5_file(file_path))\n",
        "        if index % 100 == 0:  # Affiche un message tous les 100 fichiers\n",
        "            print(f\"{process_id}: Processed {index} files\")\n",
        "    return data\n",
        "\n",
        "# Créer une liste de tous les chemins de fichiers .h5\n",
        "all_files = [os.path.join(root, file) for root, dirs, files in os.walk(root_path) for file in files if file.endswith('.h5')]\n",
        "\n",
        "# Nombre de processus à utiliser\n",
        "num_processes = 400 # Vous pouvez ajuster cela en fonction de votre CPU\n",
        "\n",
        "# Diviser la liste des fichiers en sous-ensembles pour chaque processus\n",
        "chunk_size = len(all_files) // num_processes\n",
        "file_chunks = [all_files[i:i + chunk_size] for i in range(0, len(all_files), chunk_size)]\n",
        "\n",
        "# Utiliser multiprocessing pour traiter les fichiers en parallèle\n",
        "with Pool(num_processes) as pool:\n",
        "    results = pool.map(process_files, file_chunks)\n",
        "\n",
        "# Aplatir la liste des résultats\n",
        "aggregated_data = [item for sublist in results for item in sublist]\n",
        "\n",
        "# Convertir en DataFrame et sauvegarder\n",
        "df_songs = pd.DataFrame(aggregated_data)\n",
        "df_songs.to_csv(os.path.join(root_path, \"songs_data.csv\"), index=False)\n"
      ],
      "metadata": {
        "id": "eXgvuSLBdxva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Nettoyage et conversion des données"
      ],
      "metadata": {
        "id": "CrR9UKa7HDp7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Charger le DataFrame\n",
        "df_songs = pd.read_csv('/content/drive/MyDrive/Projet_SIGNAL/MillionSongSubset/songs_data.csv')\n",
        "\n",
        "# Fonction pour convertir la durée en secondes en format minutes:secondes\n",
        "def convert_duration(seconds):\n",
        "    minutes = int(seconds // 60)\n",
        "    seconds = int(seconds % 60)\n",
        "    return f\"{minutes}:{seconds:02d}\"\n",
        "\n",
        "# Appliquer la fonction de conversion à la colonne 'duration'\n",
        "df_songs['duration'] = df_songs['duration'].apply(convert_duration)\n",
        "\n",
        "# Nettoyage des données\n",
        "# Supprimer les lignes où song_hotttnesss ou duration sont NaN\n",
        "df_songs.dropna(subset=['song_hotttnesss', 'duration'], inplace=True)\n",
        "\n",
        "# Sauvegarder le DataFrame nettoyé pour une utilisation ultérieure\n",
        "df_songs.to_csv('/content/drive/MyDrive/Projet_SIGNAL/MillionSongSubset/songs_data_cleaned.csv', index=False)\n"
      ],
      "metadata": {
        "id": "7M658rVfHEcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Création des données interactions utilisateurs-musiques"
      ],
      "metadata": {
        "id": "PlkiM3obHGmB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Charger les données des chansons nettoyées\n",
        "df_songs = pd.read_csv('/content/drive/MyDrive/Laval/Projet_SIGNAL/MillionSongSubset/songs_data_cleaned.csv')\n",
        "\n",
        "# Charger les données d'écoute des utilisateurs filtrées\n",
        "df_users_filtered = pd.read_csv('/content/drive/MyDrive/Laval/Projet_SIGNAL/MillionSongSubset/all_users_filtered.csv')\n",
        "\n",
        "# Regrouper les données par utilisateur et créer une liste des chansons écoutées par chaque utilisateur\n",
        "grouped_user_songs = df_users_filtered.groupby('user_id')['song_id'].apply(list)\n",
        "\n",
        "# Initialiser les listes pour stocker les données des utilisateurs\n",
        "user_interactions = []\n",
        "\n",
        "# Pour chaque utilisateur, créer des interactions avec les chansons qu'il a écoutées\n",
        "for user_id, song_ids in grouped_user_songs.items():\n",
        "    # Obtenir les informations sur les chansons écoutées\n",
        "    user_songs_info = df_songs[df_songs['song_id'].isin(song_ids)]\n",
        "\n",
        "    # Ajouter les informations à la liste\n",
        "    for _, song_row in user_songs_info.iterrows():\n",
        "        user_interactions.append({\n",
        "            'user_id': user_id,\n",
        "            'song_id': song_row['song_id'],\n",
        "            'artist_name': song_row['artist_name'],\n",
        "            'duration': song_row['duration'],\n",
        "            'song_hotttnesss': song_row['song_hotttnesss']\n",
        "        })\n",
        "\n",
        "# Créer un DataFrame pour les interactions utilisateurs\n",
        "df_user_interactions = pd.DataFrame(user_interactions)\n",
        "\n",
        "# Sauvegarder le DataFrame des interactions\n",
        "df_user_interactions.to_csv('/content/drive/MyDrive/Laval/Projet_SIGNAL/MillionSongSubset/user_interactions_with_duration_hotness.csv', index=False)\n"
      ],
      "metadata": {
        "id": "dT2yBEG7HHOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implémentation de l'Autoencodeur Superficiel EASE sur MSD\n",
        "## Transformation des données d'interactions en une matrice utilisateur-item"
      ],
      "metadata": {
        "id": "wKTz-8o_HInY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Charger les données d'interactions enrichies\n",
        "df_interactions = pd.read_csv('/content/drive/MyDrive/Projet_SIGNAL/MillionSongSubset/user_interactions_with_duration_hotness.csv')\n",
        "\n",
        "# Fonction pour convertir la durée en secondes\n",
        "def convert_duration_to_seconds(duration_str):\n",
        "    minutes, seconds = map(int, duration_str.split(':'))\n",
        "    return minutes * 60 + seconds\n",
        "\n",
        "# Convertir la durée en secondes et calculer le poids\n",
        "df_interactions['duration_seconds'] = df_interactions['duration'].apply(convert_duration_to_seconds)\n",
        "df_interactions['weight'] = df_interactions['duration_seconds'] * df_interactions['song_hotttnesss']\n",
        "\n",
        "# Mappage des identifiants aux indices de la matrice\n",
        "user_ids = df_interactions['user_id'].unique()\n",
        "song_ids = df_interactions['song_id'].unique()\n",
        "user_id_to_index = {user_id: index for index, user_id in enumerate(user_ids)}\n",
        "song_id_to_index = {song_id: index for index, song_id in enumerate(song_ids)}\n",
        "\n",
        "# Initialiser la matrice globale\n",
        "num_users = len(user_ids)\n",
        "num_songs = len(song_ids)\n",
        "global_matrix = np.zeros((num_users, num_songs))\n",
        "\n",
        "# Traitement par batches\n",
        "batch_size = 10000  # Ajustez en fonction de votre mémoire disponible\n",
        "for start in range(0, len(df_interactions), batch_size):\n",
        "    end = min(start + batch_size, len(df_interactions))\n",
        "    batch = df_interactions.iloc[start:end]\n",
        "    for _, row in batch.iterrows():\n",
        "        user_idx = user_id_to_index[row['user_id']]\n",
        "        song_idx = song_id_to_index[row['song_id']]\n",
        "        weight = row['weight']\n",
        "        global_matrix[user_idx, song_idx] += weight\n",
        "\n",
        "# Convertir la matrice globale en DataFrame et réinitialiser l'index\n",
        "global_df = pd.DataFrame(global_matrix, index=user_ids, columns=song_ids).reset_index()\n",
        "global_df.rename(columns={'index': 'user_id'}, inplace=True)\n",
        "\n",
        "# Réordonner les colonnes pour que 'user_id' soit la première\n",
        "column_order = ['user_id'] + list(song_ids)\n",
        "global_df = global_df[column_order]\n",
        "\n",
        "# Enregistrer le DataFrame dans un fichier CSV\n",
        "global_df.to_csv('/content/drive/MyDrive/Projet_SIGNAL/MillionSongSubset/user_item_interactions.csv', index=False)\n",
        "\n",
        "print(\"Le fichier CSV user_item_interactions.csv a été créé avec succès.\")\n"
      ],
      "metadata": {
        "id": "IvQqYd1p1lIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Construction de la Matrice de Cooccurrence des Items"
      ],
      "metadata": {
        "id": "jaBZpZfMHNnz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Charger la matrice utilisateur-item pondérée depuis un fichier .csv\n",
        "user_item_matrix = pd.read_csv('/content/drive/MyDrive/Laval/Projet_SIGNAL/MillionSongSubset/user_item_interactions.csv')\n",
        "\n",
        "# Calculer la matrice de cooccurrence des items\n",
        "item_cooccurrence_matrix = np.dot(user_item_matrix.T, user_item_matrix)\n",
        "np.fill_diagonal(item_cooccurrence_matrix, 0)  # Remplir la diagonale avec des zéros\n",
        "pd.save_csv('/content/drive/MyDrive/Laval/Projet_SIGNAL/MillionSongSubset/item_cooccurrence_matrix.csv', item_cooccurrence_matrix)"
      ],
      "metadata": {
        "id": "m4MY5NejHN7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Résolution de la Régression Ridge"
      ],
      "metadata": {
        "id": "G_Ks87u0HPKI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def ease_regression(item_cooccurrence_matrix, lambda_):\n",
        "    \"\"\"\n",
        "    Résout la régression ridge pour le modèle EASE.\n",
        "\n",
        "    :param item_cooccurrence_matrix: Matrice de cooccurrence des items.\n",
        "    :param lambda_: Paramètre de régularisation.\n",
        "    :return: Matrice de poids du modèle EASE.\n",
        "    \"\"\"\n",
        "    m, n = item_cooccurrence_matrix.shape\n",
        "    identity_matrix = np.identity(n)\n",
        "    ridge_reg_matrix = item_cooccurrence_matrix + lambda_ * identity_matrix\n",
        "    inverse_matrix = np.linalg.inv(ridge_reg_matrix)\n",
        "    b_matrix = inverse_matrix / -np.diag(inverse_matrix)\n",
        "    np.fill_diagonal(b_matrix, 0)\n",
        "    return b_matrix\n",
        "\n",
        "# Charger la matrice de cooccurrence des items depuis un fichier .csv\n",
        "item_cooccurrence_matrix = pd.read_csv('/content/drive/MyDrive/Laval/Projet_SIGNAL/MillionSongSubset/item_cooccurrence_matrix.csv', index_col=0)\n",
        "\n",
        "# Conversion de DataFrame en NumPy Array pour la régression\n",
        "item_cooccurrence_matrix_np = item_cooccurrence_matrix.to_numpy()\n",
        "\n",
        "# Appliquer la régression ridge pour obtenir la matrice de poids du modèle EASE\n",
        "lambda_ = 100  # Paramètre de régularisation\n",
        "ease_weights_np = ease_regression(item_cooccurrence_matrix_np, lambda_)\n",
        "\n",
        "# Convertir la matrice de poids NumPy en DataFrame\n",
        "ease_weights_df = pd.DataFrame(ease_weights_np, index=item_cooccurrence_matrix.columns, columns=item_cooccurrence_matrix.columns)\n",
        "\n",
        "# Enregistrer la matrice de poids EASE dans un fichier CSV\n",
        "ease_weights_df.to_csv('/content/drive/MyDrive/Laval/Projet_SIGNAL/MillionSongSubset/ease_weights.csv')\n"
      ],
      "metadata": {
        "id": "eY_Ls3bFHP6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Génération des Recommandations"
      ],
      "metadata": {
        "id": "_ad15n-jHRxg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Charger les matrices\n",
        "user_item_matrix = np.load('/content/drive/MyDrive/Laval/Projet_SIGNAL/MillionSongSubset/user_item_matrix.npy')\n",
        "item_cooccurrence_matrix = np.load('/content/drive/MyDrive/Laval/Projet_SIGNAL/MillionSongSubset/item_cooccurrence_matrix.npy')\n",
        "ease_weights = np.load('/content/drive/MyDrive/Laval/Projet_SIGNAL/MillionSongSubset/ease_weights.npy')\n",
        "\n",
        "# Générer des recommandations pour chaque utilisateur\n",
        "user_recommendations = np.dot(user_item_matrix, ease_weights)\n",
        "\n",
        "# Supposons que vous ayez des listes user_ids et song_ids\n",
        "# df_recommendations = pd.DataFrame(user_recommendations, index=user_ids, columns=song_ids)\n",
        "\n",
        "# Exemple de génération d'index et de colonnes (à remplacer par vos données réelles)\n",
        "user_ids = [f'user_{i}' for i in range(user_item_matrix.shape[0])]\n",
        "song_ids = [f'song_{i}' for i in range(user_item_matrix.shape[1])]\n",
        "\n",
        "# Convertir les recommandations en DataFrame\n",
        "df_recommendations = pd.DataFrame(user_recommendations, index=user_ids, columns=song_ids)\n",
        "\n",
        "# Filtrer et limiter les recommandations pour chaque utilisateur\n",
        "filtered_recommendations = pd.DataFrame()\n",
        "\n",
        "for user_id in df_recommendations.index:\n",
        "    # Récupérer les chansons déjà écoutées\n",
        "    listened_songs = set(df_interactions[df_interactions['user_id'] == user_id]['song_id'])\n",
        "\n",
        "    # Récupérer les recommandations pour cet utilisateur\n",
        "    user_recs = df_recommendations.loc[user_id]\n",
        "\n",
        "    # Filtrer les chansons déjà écoutées\n",
        "    user_recs_filtered = user_recs[~user_recs.index.isin(listened_songs)]\n",
        "\n",
        "    # Trier et limiter à 100 recommandations\n",
        "    top_100_recs = user_recs_filtered.sort_values(ascending=False).head(100)\n",
        "\n",
        "    # Ajouter au DataFrame\n",
        "    filtered_recommendations[user_id] = top_100_recs\n",
        "\n",
        "# Transposer le DataFrame pour un meilleur format\n",
        "filtered_recommendations = filtered_recommendations.T\n",
        "\n",
        "# Sauvegarder les recommandations filtrées\n",
        "filtered_recommendations.to_csv('/content/drive/MyDrive/Laval/Projet_SIGNAL/MillionSongSubset/filtered_recommendations_real_users.csv')\n"
      ],
      "metadata": {
        "id": "fIgX5E9OHSbb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Évaluation et Ajustement"
      ],
      "metadata": {
        "id": "WRHSP3pAHXhs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Chemin des fichiers\n",
        "user_item_matrix_path = '/content/drive/MyDrive/Laval/Projet_SIGNAL/MillionSongSubset/user_item_interactions.csv'\n",
        "songs_data_path = '/content/drive/MyDrive/Laval/Projet_SIGNAL/MillionSongSubset/songs_data_cleaned.csv'\n",
        "\n",
        "# Charger la matrice utilisateur-item\n",
        "user_item_matrix_np = np.load(user_item_matrix_path)\n",
        "\n",
        "# Supposons que vous ayez les listes user_ids et song_ids\n",
        "# user_ids = [...]\n",
        "# song_ids = [...]\n",
        "\n",
        "# Convertir la matrice NumPy en DataFrame Pandas\n",
        "user_item_matrix = pd.DataFrame(user_item_matrix_np, index=user_ids, columns=song_ids)\n",
        "\n",
        "# Charger les données des chansons\n",
        "df_songs = pd.read_csv(songs_data_path)\n",
        "\n",
        "# Fonction pour convertir la durée en secondes\n",
        "def convert_duration_to_seconds(duration):\n",
        "    if isinstance(duration, float) or isinstance(duration, int):\n",
        "        return duration\n",
        "    else:\n",
        "        minutes, seconds = map(int, duration.split(':'))\n",
        "        return minutes * 60 + seconds\n",
        "\n",
        "# Convertir la durée et remplir les valeurs manquantes pour la hotness\n",
        "df_songs['duration_in_seconds'] = df_songs['duration'].apply(convert_duration_to_seconds)\n",
        "df_songs['song_hotttnesss'] = df_songs['song_hotttnesss'].fillna(0)\n",
        "\n",
        "# Normaliser les données de durée et de hotness\n",
        "max_duration = df_songs['duration_in_seconds'].max()\n",
        "min_duration = df_songs['duration_in_seconds'].min()\n",
        "df_songs['normalized_duration'] = (df_songs['duration_in_seconds'] - min_duration) / (max_duration - min_duration)\n",
        "\n",
        "max_hotness = df_songs['song_hotttnesss'].max()\n",
        "min_hotness = df_songs['song_hotttnesss'].min()\n",
        "df_songs['normalized_hotness'] = (df_songs['song_hotttnesss'] - min_hotness) / (max_hotness - min_hotness)\n",
        "\n",
        "# Combiner durée normalisée et hotness normalisée pour créer une mesure de pertinence\n",
        "df_songs['combined_relevance'] = (df_songs['normalized_duration'] + df_songs['normalized_hotness']) / 2\n",
        "\n",
        "# Créer un dictionnaire de pertinence\n",
        "relevance_dict = df_songs.set_index('song_id')['combined_relevance'].to_dict()\n",
        "\n",
        "# Transformer la matrice utilisateur-item en dictionnaire d'interactions avec pertinence normalisée\n",
        "actual_interactions = {\n",
        "    user_id: {song_id: relevance_dict.get(song_id, 0)\n",
        "              for song_id in user_item_matrix.columns[(user_item_matrix.loc[user_id] > 0)]}\n",
        "    for user_id in user_item_matrix.index\n",
        "}\n",
        "\n",
        "# Affichage des données de pertinence normalisées\n",
        "print(relevance_dict)\n"
      ],
      "metadata": {
        "id": "bVDNDaDZAyZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calcul DCG"
      ],
      "metadata": {
        "id": "OLW-gnI5dLT6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_dcg(recommended_items, relevance, k):\n",
        "    dcg = 0\n",
        "    for i in range(k):\n",
        "        item_relevance = relevance.get(recommended_items[i], 0)\n",
        "        dcg += item_relevance / np.log2(i + 2)  # i+2 car l'index commence à 0 et le log de 1 est 0\n",
        "    return dcg\n",
        "\n",
        "def calculate_ndcg(recommended_items, relevance, k):\n",
        "    dcg = calculate_dcg(recommended_items, relevance_dict, k)\n",
        "    idcg = calculate_dcg(sorted(recommended_items, key=lambda x: relevance_dict.get(x, 0), reverse=True), relevance_dict, k)\n",
        "    return dcg / idcg if idcg > 0 else 0"
      ],
      "metadata": {
        "id": "7NtGp8F9A0II"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Affichage score NDCG"
      ],
      "metadata": {
        "id": "diLGgCQPdNHr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# Charger les recommandations EASE\n",
        "df_recommendations = pd.read_csv('/content/drive/MyDrive/Projet_SIGNAL/MillionSongSubset/filtered_recommendations_real_users.csv', index_col=0)\n",
        "\n",
        "ndcg_scores = []\n",
        "k = 100  # Nombre de recommandations à considérer\n",
        "\n",
        "for user_id in df_recommendations.index:\n",
        "    recommended_items = df_recommendations.loc[user_id].sort_values(ascending=False).index.tolist()[:k]\n",
        "    ndcg_score = calculate_ndcg(recommended_items, relevance_dict, k)\n",
        "    ndcg_scores.append(ndcg_score)\n",
        "\n",
        "# Calculate NDCG score\n",
        "ease_average_ndcg = np.mean(ndcg_scores)\n",
        "ease_variance_ndcg = np.var(ndcg_scores)\n",
        "print(f\"Mean : {ease_average_ndcg:.5f} - Variance : {ease_variance_ndcg:.5f}\")"
      ],
      "metadata": {
        "id": "ny6pEM8TA3Da"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}